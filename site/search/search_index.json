{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Hello! I am Kironte and this is the Roblox Neural Network Library, the second iteration of my NN library work. Here you will find thorough explanations on machine learning, the module, it's documentation, and example code that will set you up for your machine learning programming. For a proper (and lengthy) introduction to machine learning and the library, head here .","title":"Home"},{"location":"#home","text":"Hello! I am Kironte and this is the Roblox Neural Network Library, the second iteration of my NN library work. Here you will find thorough explanations on machine learning, the module, it's documentation, and example code that will set you up for your machine learning programming. For a proper (and lengthy) introduction to machine learning and the library, head here .","title":"Home"},{"location":"exampleCode/","text":"Example Code Along with the documentation, you will need some examples of what your code should look like when using this library. The code needed to operate this library is significantly simpler and more readable than the one for the first library, but it still has the same basic principles. For consistency, this example code does the same thing as the previous library's example code. The 2 examples shows how to operate single networks as well as genetic algorithm populations. LSTM and feedforward networks are interchangeable here, assuming you know how to use the former as they are not for novices. Each example is heavily documented for your understanding. To use these examples, simply drop the library into ReplicatedStorage, and copy paste the example code into a server/localscript anywhere you like, and run. Both examples have undocumented versions if you find it distracting/bloating. Single Network Example --Whenever using math.random() for small experiments like this, you should set a random seed --in order to not get the same results every time. math.randomseed ( os.clock () + os.time ()) --For this experiment, I placed the library package in ReplicatedStorage. local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local Momentum = require ( Package . Optimizer . Momentum ) --If the training/testing is intensive, we will want to setup automatic wait() statements --in order to avoid a timeout. This can be done with os.clock(). local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- --This setting dictionary contains whatever customizations you want on your neural network. --Each setting has it's own default and is completely optional. local setting = { --We will set the optimizer to Momentum because it seems to work the best for this experiment. Optimizer = Momentum . new (); --We want to accept negative inputs so a LeakyReLU will do. HiddenActivationName = \"LeakyReLU\" ; --The output is between 0 and 1 so good ol' sigmoid will do. OutputActivationName = \"Sigmoid\" ; LearningRate = 0.3 ; } --Number of generations for the training. local generations = 200000 --Number of generations that need to pass before we backpropagate the network. local numOfGenerationsBeforeLearning = 1 ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- --This is the statement that creates the network. Only the most basic settings are used here. --The rest are in the 'setting' dictionary. In this case, the network has 2 inputs 'x' and 'y', --2 layers with 2 nodes each, and 1 output 'out'. local net = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) --To backpropagate, you need to get the network's backpropagator. local backProp = net : GetBackPropagator () --This function determines what mathematical function we want to test the network with --and if the given coordinates are above (1) or below (0) the function. --For this experiment, a simple cubic will do. function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end for generation = 1 , generations do --Both input and outputs are dictionaries where the key (index) is the name of the input/output, --while the value is it's associated value. This is why 'coords' has x and y, while 'correctAnswer' --has out. local coords = { x = math.random ( - 400 , 400 ) / 100 , y = math.random ( - 400 , 400 ) / 100 } local correctAnswer = { out = isAboveFunction ( coords . x , coords . y )} --Here, we calculate the cost of the network with the given inputs and correct outputs. Basically, --we calculate how wrong/right the network currently is. backProp : CalculateCost ( coords , correctAnswer ) --The automated wait() and print() statements that give the computer a break every 0.1 seconds and --give us an update as to how much of training is left. if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( generation / generations * ( 100 ) .. \"% trained. Cost: \" .. backProp : GetTotalCost ()) end --If time is of the essence, you can have the backpropagator save the costs of multiple generations --before actually training the network. This results in less effective training, but is way faster. if generation % numOfGenerationsBeforeLearning == 0 then backProp : Learn () end end --Total number of test runs. local totalRuns = 0 --The number of runs that were deemed correct. local wins = 0 --Lua is dumb and counts -400 to 400 as 801 runs instead of 800 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } --Now, you can get the output of a network by just calling it like a function. local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) --I will call it correct if the difference between the correct answer and the network's output --is less than or equal to 0.3. if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" ) GeneticAlgorithm Example --Whenever using math.random() for small experiments like this, you should set a random seed --in order to not get the same results every time. math.randomseed ( os.clock () + os.time ()) --For this experiment, I placed the library package in ReplicatedStorage. local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local ParamEvo = require ( Package . GeneticAlgorithm . ParamEvo ) local Momentum = require ( Package . Optimizer . Momentum ) --If the training/testing is intensive, we will want to setup automatic wait() statements --in order to avoid a timeout. This can be done with os.clock(). local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- --Number of generations to run. local generations = 30 --The number of networks in the population. More means better outcome but worse performance. local population = 20 --This function determines what mathematical function we want to test the network with --and if the given coordinates are above (1) or below (0) the function. --For this experiment, a simple cubic will do. function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end --This setting dictionary contains whatever customizations you want on the neural network --that will act as a template for the population. --Each setting has it's own default and is completely optional. local setting = { --We want to accept negative inputs so a LeakyReLU will do. HiddenActivationName = \"LeakyReLU\" ; --The output is between 0 and 1 so good ol' sigmoid will do. OutputActivationName = \"Sigmoid\" ; } --Similar to 'setting', this dictionary contains the settings for the genetic algorithm. --As before, every setting has a default and is completely optional. local geneticSetting = { --The function that, when given the network, will return it's score. ScoreFunction = function ( net ) local score = 0 --Lua is dumb and counts -400 to 400 as 801 runs instead of 800 for x = - 400 , 399 , 8 do for y = - 400 , 399 , 8 do --We want the values to stay within -4 and 4. local coords = { x = x / 100 , y = y / 100 } local correctAnswer = isAboveFunction ( coords . x , coords . y ) --Now, you can get the output of a network by just calling it like a function. local output = net ( coords ) --I will call it correct if the difference between the correct answer and the network's output --is less than or equal to 0.3. if math.abs ( output . out - correctAnswer ) <= 0.3 then score += 1 end end if os.clock () - clock >= 0.1 then clock = os.clock () wait () end end return score end ; --The function that runs when a generation is complete. It is given the genetic algorithm as input. PostFunction = function ( geneticAlgo ) local info = geneticAlgo : GetInfo () print ( \"Generation \" .. info . Generation .. \", Best Score: \" .. info . BestScore / ( 100 ) ^ 2 * ( 100 ) .. \"%\" ) end ; } ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- --This is the statement that creates the template network. Only the most basic settings are used here. --The rest are in the 'setting' dictionary. In this case, the network has 2 inputs 'x' and 'y', --2 layers with 2 nodes each, and 1 output 'out'. Each network in the population will have this --same structure. local tempNet = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) local geneticAlgo = ParamEvo . new ( tempNet , population , geneticSetting ) --We just tell the genetic algorithm to process a set number of generations and it does the rest --of the work for us. geneticAlgo : ProcessGenerations ( generations ) --For testing, we go ahead and grab the first network in the population. local net = geneticAlgo : GetBestNetwork () --Total number of test runs. local totalRuns = 0 --The number of runs that were deemed correct. local wins = 0 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" ) Single Network Example (Undocumented) math.randomseed ( os.clock () + os.time ()) local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local Momentum = require ( Package . Optimizer . Momentum ) local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- local setting = { Optimizer = Momentum . new (); HiddenActivationName = \"LeakyReLU\" ; OutputActivationName = \"Sigmoid\" ; LearningRate = 0.3 ; } local generations = 200000 local numOfGenerationsBeforeLearning = 1 ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- local net = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) local backProp = net : GetBackPropagator () function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end for generation = 1 , generations do local coords = { x = math.random ( - 400 , 400 ) / 100 , y = math.random ( - 400 , 400 ) / 100 } local correctAnswer = { out = isAboveFunction ( coords . x , coords . y )} backProp : CalculateCost ( coords , correctAnswer ) if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( generation / generations * ( 100 ) .. \"% trained. Cost: \" .. backProp : GetTotalCost ()) end if generation % numOfGenerationsBeforeLearning == 0 then backProp : Learn () end end local totalRuns = 0 local wins = 0 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" ) GeneticAlgorithm Example (Undocumented) math.randomseed ( os.clock () + os.time ()) local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local ParamEvo = require ( Package . GeneticAlgorithm . ParamEvo ) local Momentum = require ( Package . Optimizer . Momentum ) local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- local generations = 30 local population = 20 function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end local setting = { HiddenActivationName = \"LeakyReLU\" ; OutputActivationName = \"Sigmoid\" ; } local geneticSetting = { ScoreFunction = function ( net ) local score = 0 for x = - 400 , 399 , 8 do for y = - 400 , 399 , 8 do local coords = { x = x / 100 , y = y / 100 } local correctAnswer = isAboveFunction ( coords . x , coords . y ) local output = net ( coords ) if math.abs ( output . out - correctAnswer ) <= 0.3 then score += 1 end end if os.clock () - clock >= 0.1 then clock = os.clock () wait () end end return score end ; PostFunction = function ( geneticAlgo ) local info = geneticAlgo : GetInfo () print ( \"Generation \" .. info . Generation .. \", Best Score: \" .. info . BestScore / ( 100 ) ^ 2 * ( 100 ) .. \"%\" ) end ; } ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- local tempNet = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) local geneticAlgo = ParamEvo . new ( tempNet , population , geneticSetting ) geneticAlgo : ProcessGenerations ( generations ) local net = geneticAlgo : GetBestNetwork () local totalRuns = 0 local wins = 0 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" )","title":"Example Code"},{"location":"exampleCode/#example-code","text":"Along with the documentation, you will need some examples of what your code should look like when using this library. The code needed to operate this library is significantly simpler and more readable than the one for the first library, but it still has the same basic principles. For consistency, this example code does the same thing as the previous library's example code. The 2 examples shows how to operate single networks as well as genetic algorithm populations. LSTM and feedforward networks are interchangeable here, assuming you know how to use the former as they are not for novices. Each example is heavily documented for your understanding. To use these examples, simply drop the library into ReplicatedStorage, and copy paste the example code into a server/localscript anywhere you like, and run. Both examples have undocumented versions if you find it distracting/bloating.","title":"Example Code"},{"location":"exampleCode/#single-network-example","text":"--Whenever using math.random() for small experiments like this, you should set a random seed --in order to not get the same results every time. math.randomseed ( os.clock () + os.time ()) --For this experiment, I placed the library package in ReplicatedStorage. local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local Momentum = require ( Package . Optimizer . Momentum ) --If the training/testing is intensive, we will want to setup automatic wait() statements --in order to avoid a timeout. This can be done with os.clock(). local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- --This setting dictionary contains whatever customizations you want on your neural network. --Each setting has it's own default and is completely optional. local setting = { --We will set the optimizer to Momentum because it seems to work the best for this experiment. Optimizer = Momentum . new (); --We want to accept negative inputs so a LeakyReLU will do. HiddenActivationName = \"LeakyReLU\" ; --The output is between 0 and 1 so good ol' sigmoid will do. OutputActivationName = \"Sigmoid\" ; LearningRate = 0.3 ; } --Number of generations for the training. local generations = 200000 --Number of generations that need to pass before we backpropagate the network. local numOfGenerationsBeforeLearning = 1 ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- --This is the statement that creates the network. Only the most basic settings are used here. --The rest are in the 'setting' dictionary. In this case, the network has 2 inputs 'x' and 'y', --2 layers with 2 nodes each, and 1 output 'out'. local net = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) --To backpropagate, you need to get the network's backpropagator. local backProp = net : GetBackPropagator () --This function determines what mathematical function we want to test the network with --and if the given coordinates are above (1) or below (0) the function. --For this experiment, a simple cubic will do. function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end for generation = 1 , generations do --Both input and outputs are dictionaries where the key (index) is the name of the input/output, --while the value is it's associated value. This is why 'coords' has x and y, while 'correctAnswer' --has out. local coords = { x = math.random ( - 400 , 400 ) / 100 , y = math.random ( - 400 , 400 ) / 100 } local correctAnswer = { out = isAboveFunction ( coords . x , coords . y )} --Here, we calculate the cost of the network with the given inputs and correct outputs. Basically, --we calculate how wrong/right the network currently is. backProp : CalculateCost ( coords , correctAnswer ) --The automated wait() and print() statements that give the computer a break every 0.1 seconds and --give us an update as to how much of training is left. if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( generation / generations * ( 100 ) .. \"% trained. Cost: \" .. backProp : GetTotalCost ()) end --If time is of the essence, you can have the backpropagator save the costs of multiple generations --before actually training the network. This results in less effective training, but is way faster. if generation % numOfGenerationsBeforeLearning == 0 then backProp : Learn () end end --Total number of test runs. local totalRuns = 0 --The number of runs that were deemed correct. local wins = 0 --Lua is dumb and counts -400 to 400 as 801 runs instead of 800 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } --Now, you can get the output of a network by just calling it like a function. local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) --I will call it correct if the difference between the correct answer and the network's output --is less than or equal to 0.3. if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" )","title":"Single Network Example"},{"location":"exampleCode/#geneticalgorithm-example","text":"--Whenever using math.random() for small experiments like this, you should set a random seed --in order to not get the same results every time. math.randomseed ( os.clock () + os.time ()) --For this experiment, I placed the library package in ReplicatedStorage. local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local ParamEvo = require ( Package . GeneticAlgorithm . ParamEvo ) local Momentum = require ( Package . Optimizer . Momentum ) --If the training/testing is intensive, we will want to setup automatic wait() statements --in order to avoid a timeout. This can be done with os.clock(). local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- --Number of generations to run. local generations = 30 --The number of networks in the population. More means better outcome but worse performance. local population = 20 --This function determines what mathematical function we want to test the network with --and if the given coordinates are above (1) or below (0) the function. --For this experiment, a simple cubic will do. function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end --This setting dictionary contains whatever customizations you want on the neural network --that will act as a template for the population. --Each setting has it's own default and is completely optional. local setting = { --We want to accept negative inputs so a LeakyReLU will do. HiddenActivationName = \"LeakyReLU\" ; --The output is between 0 and 1 so good ol' sigmoid will do. OutputActivationName = \"Sigmoid\" ; } --Similar to 'setting', this dictionary contains the settings for the genetic algorithm. --As before, every setting has a default and is completely optional. local geneticSetting = { --The function that, when given the network, will return it's score. ScoreFunction = function ( net ) local score = 0 --Lua is dumb and counts -400 to 400 as 801 runs instead of 800 for x = - 400 , 399 , 8 do for y = - 400 , 399 , 8 do --We want the values to stay within -4 and 4. local coords = { x = x / 100 , y = y / 100 } local correctAnswer = isAboveFunction ( coords . x , coords . y ) --Now, you can get the output of a network by just calling it like a function. local output = net ( coords ) --I will call it correct if the difference between the correct answer and the network's output --is less than or equal to 0.3. if math.abs ( output . out - correctAnswer ) <= 0.3 then score += 1 end end if os.clock () - clock >= 0.1 then clock = os.clock () wait () end end return score end ; --The function that runs when a generation is complete. It is given the genetic algorithm as input. PostFunction = function ( geneticAlgo ) local info = geneticAlgo : GetInfo () print ( \"Generation \" .. info . Generation .. \", Best Score: \" .. info . BestScore / ( 100 ) ^ 2 * ( 100 ) .. \"%\" ) end ; } ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- --This is the statement that creates the template network. Only the most basic settings are used here. --The rest are in the 'setting' dictionary. In this case, the network has 2 inputs 'x' and 'y', --2 layers with 2 nodes each, and 1 output 'out'. Each network in the population will have this --same structure. local tempNet = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) local geneticAlgo = ParamEvo . new ( tempNet , population , geneticSetting ) --We just tell the genetic algorithm to process a set number of generations and it does the rest --of the work for us. geneticAlgo : ProcessGenerations ( generations ) --For testing, we go ahead and grab the first network in the population. local net = geneticAlgo : GetBestNetwork () --Total number of test runs. local totalRuns = 0 --The number of runs that were deemed correct. local wins = 0 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" )","title":"GeneticAlgorithm Example"},{"location":"exampleCode/#single-network-example-undocumented","text":"math.randomseed ( os.clock () + os.time ()) local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local Momentum = require ( Package . Optimizer . Momentum ) local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- local setting = { Optimizer = Momentum . new (); HiddenActivationName = \"LeakyReLU\" ; OutputActivationName = \"Sigmoid\" ; LearningRate = 0.3 ; } local generations = 200000 local numOfGenerationsBeforeLearning = 1 ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- local net = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) local backProp = net : GetBackPropagator () function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end for generation = 1 , generations do local coords = { x = math.random ( - 400 , 400 ) / 100 , y = math.random ( - 400 , 400 ) / 100 } local correctAnswer = { out = isAboveFunction ( coords . x , coords . y )} backProp : CalculateCost ( coords , correctAnswer ) if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( generation / generations * ( 100 ) .. \"% trained. Cost: \" .. backProp : GetTotalCost ()) end if generation % numOfGenerationsBeforeLearning == 0 then backProp : Learn () end end local totalRuns = 0 local wins = 0 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" )","title":"Single Network Example (Undocumented)"},{"location":"exampleCode/#geneticalgorithm-example-undocumented","text":"math.randomseed ( os.clock () + os.time ()) local Package = game : GetService ( \"ReplicatedStorage\" ). NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local ParamEvo = require ( Package . GeneticAlgorithm . ParamEvo ) local Momentum = require ( Package . Optimizer . Momentum ) local clock = os.clock () ----------------<<MAIN SETTINGS>>--------------------------------------------------------------- local generations = 30 local population = 20 function isAboveFunction ( x , y ) if x ^ 3 + 2 * x ^ 2 < y then return 0 end return 1 end local setting = { HiddenActivationName = \"LeakyReLU\" ; OutputActivationName = \"Sigmoid\" ; } local geneticSetting = { ScoreFunction = function ( net ) local score = 0 for x = - 400 , 399 , 8 do for y = - 400 , 399 , 8 do local coords = { x = x / 100 , y = y / 100 } local correctAnswer = isAboveFunction ( coords . x , coords . y ) local output = net ( coords ) if math.abs ( output . out - correctAnswer ) <= 0.3 then score += 1 end end if os.clock () - clock >= 0.1 then clock = os.clock () wait () end end return score end ; PostFunction = function ( geneticAlgo ) local info = geneticAlgo : GetInfo () print ( \"Generation \" .. info . Generation .. \", Best Score: \" .. info . BestScore / ( 100 ) ^ 2 * ( 100 ) .. \"%\" ) end ; } ----------------<<END OF MAIN SETTINGS>>--------------------------------------------------------------- local tempNet = FeedforwardNetwork . new ({ \"x\" , \"y\" }, 2 , 3 ,{ \"out\" }, setting ) local geneticAlgo = ParamEvo . new ( tempNet , population , geneticSetting ) geneticAlgo : ProcessGenerations ( generations ) local net = geneticAlgo : GetBestNetwork () local totalRuns = 0 local wins = 0 for x = - 400 , 399 do for y = - 400 , 399 do local coords = { x = x / 100 , y = y / 100 } local output = net ( coords ) local correctAnswer = isAboveFunction ( coords . x , coords . y ) if math.abs ( output . out - correctAnswer ) <= 0.3 then wins += 1 end totalRuns += 1 end if os.clock () - clock >= 0.1 then clock = os.clock () wait () print ( \"Testing... \" .. ( x + 400 ) / ( 8 ) .. \"%\" ) end end print ( wins / totalRuns * ( 100 ) .. \"% correct!\" )","title":"GeneticAlgorithm Example (Undocumented)"},{"location":"install/","text":"Installation Procedure The Github page is not meant to install the library, hence why I am not using Rojo. To install this library and use it yourself, you need to grab it from the catalog here . The package should be in either ReplicatedStorage, or ServerStorage, depending on where you intend to use it. If you're not sure, just place it in ReplicatedStorage in some folder where you store your 3rd party modules. And... thats it! This folder contains every module necessary for the library to work, including the OOP implementation called \"Base\". In order to use it in a script, you need to require() the Base module along with any classes you plan to use, like the neural network classes or the optimizer classes. For most packages, BaseRedirect is used by the package to locate the Base module, allowing you to move it's location and only have to change a single line of code. For this package, the Base is in the same folder. local repStorage = game : GetService ( \"ReplicatedStorage\" ) --Try to keep the variable names equal to the class's name. local Package = repStorage . NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local ParamEvo = require ( Package . GeneticAlgorithm . ParamEvo ) --Your machine learning code goes here. See example code (next page) for more info!","title":"Installation"},{"location":"install/#installation-procedure","text":"The Github page is not meant to install the library, hence why I am not using Rojo. To install this library and use it yourself, you need to grab it from the catalog here . The package should be in either ReplicatedStorage, or ServerStorage, depending on where you intend to use it. If you're not sure, just place it in ReplicatedStorage in some folder where you store your 3rd party modules. And... thats it! This folder contains every module necessary for the library to work, including the OOP implementation called \"Base\". In order to use it in a script, you need to require() the Base module along with any classes you plan to use, like the neural network classes or the optimizer classes. For most packages, BaseRedirect is used by the package to locate the Base module, allowing you to move it's location and only have to change a single line of code. For this package, the Base is in the same folder. local repStorage = game : GetService ( \"ReplicatedStorage\" ) --Try to keep the variable names equal to the class's name. local Package = repStorage . NNLibrary local Base = require ( Package . BaseRedirect ) local FeedforwardNetwork = require ( Package . NeuralNetwork . FeedforwardNetwork ) local ParamEvo = require ( Package . GeneticAlgorithm . ParamEvo ) --Your machine learning code goes here. See example code (next page) for more info!","title":"Installation Procedure"},{"location":"intro/","text":"Introduction Why machine learning? Machine learning concepts allow for programmers to solve problems that are either too difficult to code manually, or are too obscure to figure out. Most famous examples of this are self-driving vehicles, bots, text to speech (and visa-versa), physics simulations (for mass scale use where normal simulation is too slow), adaptive animations, image generation, facial recognition, terrain generation, and hundreds of more. All of these are either impossible without machine learning or are greatly enhanced by it. Roblox Examples ScriptOn 's early attempts on self driving cars. ScriptOn's later attempts/successes on self driving cars. Badcc's MNIST recognition . Though not Roblox related, it would be a travesty to not include AlphaStar , the Starcraft 2 AI that defeated pro players (which uses the new type of network I researched for this library, LSTMs). Why make the library? Roblox is the perfect platform for young programmers to begin their careers (as well as further them with DevEx). Slowly but surely fundamental concepts like Object Orientated Programming (OOP) become more and more popular among the community and more newbies get to learn them. Unfortunately, this development seems to have missed the programming concept with arguably the most potential and impact on our present and future lives: Machine Learning. The ability to teach a program to adapt to it's circumstances and act to survive or otherwise achieve it's main goal on it's own. This is achieved with a concept called \"neural networks\", a network of individual nodes or neurons that work independently to bring the network to life, much like neurons in our brains. The problem is that machine learning is typically seen as something for experienced programmers or good mathematicians, things that the overwhelming majority of all programmers are not (yet). Machine learning is extremely undeveloped on Roblox and has no universal implementation on Roblox: only attempts and developers playing around on their own. After taking an interest in the concept myself, I spent a good half of a year researching neural networks and how they function, mostly because there are extremely few proper materials that explain things in an understandable jargon or manner. During my self-education, I wrote my first neural network library. It is extremely rough and unprofessionally done due to my lack of experience, and, though successful, is still an embarassment to me. After learning OOP and writing my own implementation for it in Lua, I began working on my second neural network library. After spending a good few months researching an entirely different type of network, I am left with a much better design that is actually user friendly and readable. How do neural networks work? All basic neural networks (NN) are just an assembly of nodes that are somehow plugged into each other for communication. The most common and simple network type is the feedforward network, also called a multilayer perceptron but that is irrelevant. As you can see, most networks, like the feedforward above, sort their nodes into layers. Most networks have 3 layer types: The input layer, the hidden layer(s), and the output layer. Each layer is an array of nodes, with nodes from one layer connecting to nodes in the next layer using connections called 'synapses'. Before we get to the layer types, we need to understand individual nodes. Each basic node consists of a set of 'weights', a 'bias', and an 'activation function'. Every synapse that connects to the node on the input (left, in this case) side carries the previous node's output. This output is then multiplied by the weight associated with the synapse. If the node's output is 0.6, and the weight associated with it is 0.1, than the product will be 0.06. This operation is done on every synapse connected to the node in question. Once all of these products are calculated, they are summed up along with the node's bias, a single number that regulates the node's activity positively or negatively. This sum is almost ready to be the node's output. However, we have a problem: this sum is not scaled. It can be either really low, or really high, which can cause serious problems in the network down the line. This regulate this sum, an activation function is used. This function is supposed to take in this sum and convert it into a managable number, like 0 to 1. This number then becomes the node's output, which is then propagated to the next node down the line. The most well known activation function is the sigmoid, though ReLU is the most popular and accepted. Now, what are these layer types? The first layer is always the input layer. The input layer is made up of special input nodes that are like normal nodes, expect they do not have any synapses, weights, bias, or activation functions. The only thing they have is an output, given to them at the start of propagation by the user; an input to the network. The hidden layers are the normal nodes discussed earlier. They are the core of the network's processing. The reason they are 'hidden' is because the manner of processing is that of a black box; you can feed it an input and recieve an output, but you have no way of understanding how it got the output. It just did. All you can do is punish or reward it to do better next time and train it, but thats for later. The last layer is always the output layer. The output nodes in this layer are the same as normal nodes expect that their outputs are read and given to the user when the network is finished. With that, lets head to an example. In no way do I expect you to understand all of this right off the bat; I am no teacher and people learn on their own rate. Try to reread some of these explanations as we go through the example. For this example, all biases are 0 for simplicity, and the activation function is a sigmoid, and the inputs are both set to 1. On the first node (A) in the hidden layer, we need to get the weighted sum. The first synapse has the weight of 0.2 and the second synapse has the weight of 0.2 (a bit hard to see). Since both inputs are 1, the weighted inputs are 1 x 0.2 and 1 x 0.2 respectively. To get the weighted sum, we just add them up along with the bias. 0.2 + 0.2 + 0 = 0.4. When we plug 0.4 into the sigmoid function, the output is 0.598, 0.6 for the image. This is the node's output. The process is the same for the other 2 nodes in this layer. Once the entire hidden layer has been calculated and each node having an ouput, 0.6, 0.62, and 0.6, A B and C respectively, we can calculate the output node. The process is exactly the same as before. We get the weighted sum: 0.6 x 0.3 + 0.62 x 0.2 + 0.6 x 0.1 = 0.364. When plugged into the sigmoid function, 0.364 turns into 0.590, our final output.","title":"Introduction"},{"location":"intro/#introduction","text":"","title":"Introduction"},{"location":"intro/#why-machine-learning","text":"Machine learning concepts allow for programmers to solve problems that are either too difficult to code manually, or are too obscure to figure out. Most famous examples of this are self-driving vehicles, bots, text to speech (and visa-versa), physics simulations (for mass scale use where normal simulation is too slow), adaptive animations, image generation, facial recognition, terrain generation, and hundreds of more. All of these are either impossible without machine learning or are greatly enhanced by it.","title":"Why machine learning?"},{"location":"intro/#roblox-examples","text":"ScriptOn 's early attempts on self driving cars. ScriptOn's later attempts/successes on self driving cars. Badcc's MNIST recognition . Though not Roblox related, it would be a travesty to not include AlphaStar , the Starcraft 2 AI that defeated pro players (which uses the new type of network I researched for this library, LSTMs).","title":"Roblox Examples"},{"location":"intro/#why-make-the-library","text":"Roblox is the perfect platform for young programmers to begin their careers (as well as further them with DevEx). Slowly but surely fundamental concepts like Object Orientated Programming (OOP) become more and more popular among the community and more newbies get to learn them. Unfortunately, this development seems to have missed the programming concept with arguably the most potential and impact on our present and future lives: Machine Learning. The ability to teach a program to adapt to it's circumstances and act to survive or otherwise achieve it's main goal on it's own. This is achieved with a concept called \"neural networks\", a network of individual nodes or neurons that work independently to bring the network to life, much like neurons in our brains. The problem is that machine learning is typically seen as something for experienced programmers or good mathematicians, things that the overwhelming majority of all programmers are not (yet). Machine learning is extremely undeveloped on Roblox and has no universal implementation on Roblox: only attempts and developers playing around on their own. After taking an interest in the concept myself, I spent a good half of a year researching neural networks and how they function, mostly because there are extremely few proper materials that explain things in an understandable jargon or manner. During my self-education, I wrote my first neural network library. It is extremely rough and unprofessionally done due to my lack of experience, and, though successful, is still an embarassment to me. After learning OOP and writing my own implementation for it in Lua, I began working on my second neural network library. After spending a good few months researching an entirely different type of network, I am left with a much better design that is actually user friendly and readable.","title":"Why make the library?"},{"location":"intro/#how-do-neural-networks-work","text":"All basic neural networks (NN) are just an assembly of nodes that are somehow plugged into each other for communication. The most common and simple network type is the feedforward network, also called a multilayer perceptron but that is irrelevant. As you can see, most networks, like the feedforward above, sort their nodes into layers. Most networks have 3 layer types: The input layer, the hidden layer(s), and the output layer. Each layer is an array of nodes, with nodes from one layer connecting to nodes in the next layer using connections called 'synapses'. Before we get to the layer types, we need to understand individual nodes. Each basic node consists of a set of 'weights', a 'bias', and an 'activation function'. Every synapse that connects to the node on the input (left, in this case) side carries the previous node's output. This output is then multiplied by the weight associated with the synapse. If the node's output is 0.6, and the weight associated with it is 0.1, than the product will be 0.06. This operation is done on every synapse connected to the node in question. Once all of these products are calculated, they are summed up along with the node's bias, a single number that regulates the node's activity positively or negatively. This sum is almost ready to be the node's output. However, we have a problem: this sum is not scaled. It can be either really low, or really high, which can cause serious problems in the network down the line. This regulate this sum, an activation function is used. This function is supposed to take in this sum and convert it into a managable number, like 0 to 1. This number then becomes the node's output, which is then propagated to the next node down the line. The most well known activation function is the sigmoid, though ReLU is the most popular and accepted. Now, what are these layer types? The first layer is always the input layer. The input layer is made up of special input nodes that are like normal nodes, expect they do not have any synapses, weights, bias, or activation functions. The only thing they have is an output, given to them at the start of propagation by the user; an input to the network. The hidden layers are the normal nodes discussed earlier. They are the core of the network's processing. The reason they are 'hidden' is because the manner of processing is that of a black box; you can feed it an input and recieve an output, but you have no way of understanding how it got the output. It just did. All you can do is punish or reward it to do better next time and train it, but thats for later. The last layer is always the output layer. The output nodes in this layer are the same as normal nodes expect that their outputs are read and given to the user when the network is finished. With that, lets head to an example. In no way do I expect you to understand all of this right off the bat; I am no teacher and people learn on their own rate. Try to reread some of these explanations as we go through the example. For this example, all biases are 0 for simplicity, and the activation function is a sigmoid, and the inputs are both set to 1. On the first node (A) in the hidden layer, we need to get the weighted sum. The first synapse has the weight of 0.2 and the second synapse has the weight of 0.2 (a bit hard to see). Since both inputs are 1, the weighted inputs are 1 x 0.2 and 1 x 0.2 respectively. To get the weighted sum, we just add them up along with the bias. 0.2 + 0.2 + 0 = 0.4. When we plug 0.4 into the sigmoid function, the output is 0.598, 0.6 for the image. This is the node's output. The process is the same for the other 2 nodes in this layer. Once the entire hidden layer has been calculated and each node having an ouput, 0.6, 0.62, and 0.6, A B and C respectively, we can calculate the output node. The process is exactly the same as before. We get the weighted sum: 0.6 x 0.3 + 0.62 x 0.2 + 0.6 x 0.1 = 0.364. When plugged into the sigmoid function, 0.364 turns into 0.590, our final output.","title":"How do neural networks work?"},{"location":"documentation/","text":"API Documentation","title":"API Documentation"},{"location":"documentation/#api-documentation","text":"","title":"API Documentation"},{"location":"documentation/ActivationFunction/","text":"ActivationFunction Class This class is responsible for housing the basic activation functions used in the library. As it didn't really need any extra work, this is directly ported from the first library. When choosing a function, you can only choose 1 of the supported functions below: \u2022 Identity \u2022 Binary \u2022 Sigmoid \u2022 Tanh \u2022 ArcTan \u2022 Sin \u2022 Sinc \u2022 ArSinh \u2022 SoftPlus \u2022 BentIdentity \u2022 ReLU \u2022 SoftReLU \u2022 LeakyReLU \u2022 Swish \u2022 ElliotSign \u2022 Gaussian \u2022 SQ-RBF ActivationFunction .new(string activName) Creates and returns the ActivationFunction with the !activName! indicating which activation function to use. number :Calculate(number x,bool deriv) Calculates the Y of the activation function given the !x!, or, if !deriv! is true, instead returns the derivative tangent of the function at !x!. number :GetValue(number x) Calculates the Y of the activation function given the !x!. number :GetDeriv(number x) Calculates the derivative tangent of the activation function at !x!. void :SetActivator(string activName) Sets the activation function. Be sure to review the possible options at the top of the page. string :GetActivator() Returns the currently set activation function name.","title":"ActivationFunction"},{"location":"documentation/ActivationFunction/#activationfunction-class","text":"This class is responsible for housing the basic activation functions used in the library. As it didn't really need any extra work, this is directly ported from the first library. When choosing a function, you can only choose 1 of the supported functions below: \u2022 Identity \u2022 Binary \u2022 Sigmoid \u2022 Tanh \u2022 ArcTan \u2022 Sin \u2022 Sinc \u2022 ArSinh \u2022 SoftPlus \u2022 BentIdentity \u2022 ReLU \u2022 SoftReLU \u2022 LeakyReLU \u2022 Swish \u2022 ElliotSign \u2022 Gaussian \u2022 SQ-RBF ActivationFunction .new(string activName) Creates and returns the ActivationFunction with the !activName! indicating which activation function to use. number :Calculate(number x,bool deriv) Calculates the Y of the activation function given the !x!, or, if !deriv! is true, instead returns the derivative tangent of the function at !x!. number :GetValue(number x) Calculates the Y of the activation function given the !x!. number :GetDeriv(number x) Calculates the derivative tangent of the activation function at !x!. void :SetActivator(string activName) Sets the activation function. Be sure to review the possible options at the top of the page. string :GetActivator() Returns the currently set activation function name.","title":"ActivationFunction Class"},{"location":"documentation/AdaGrad/","text":"AdaGrad Class This class is responsible for the AdaGrad optimizer. AdaGrad .new(number epsilon = 10^-8) Creates and returns the AdaGrad optimizer with the given epsilon. Inherited from Optimizer : Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"AdaGrad"},{"location":"documentation/AdaGrad/#adagrad-class","text":"This class is responsible for the AdaGrad optimizer. AdaGrad .new(number epsilon = 10^-8) Creates and returns the AdaGrad optimizer with the given epsilon.","title":"AdaGrad Class"},{"location":"documentation/AdaGrad/#inherited-from-optimizer","text":"Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Inherited from Optimizer:"},{"location":"documentation/Adam/","text":"Adam Class This class is responsible for the Adam optimizer. Adam .new(number decayConstant1 = 0.9, number decayConstant2 = 0.999, number epsilon = 10^-7) Creates and returns the Adam optimizer with the given decay constants and epsilon. Inherited from Optimizer : Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Adam"},{"location":"documentation/Adam/#adam-class","text":"This class is responsible for the Adam optimizer. Adam .new(number decayConstant1 = 0.9, number decayConstant2 = 0.999, number epsilon = 10^-7) Creates and returns the Adam optimizer with the given decay constants and epsilon.","title":"Adam Class"},{"location":"documentation/Adam/#inherited-from-optimizer","text":"Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Inherited from Optimizer:"},{"location":"documentation/BackPropagator/","text":"BackPropagator Class This class is responsible for backpropagating the associated neural network according to it's set optimizer. BackPropagator .new(NeuralNetwork neuralNetwork) Creates and returns the BackPropagator with the given !neuralNetwork!. void :Reset() Resets all saved gradients and costs. Tuple :CalculateCost(dictionary inputValues,dictionary correctOutputValues) Calculates and saves the cost values for each node when the network is ran with !inputValues! and the output is compared to the given !correctOutputValues!. This function can be called multiple times before :Learn() as it will simply average the cost values over time.// dictionary CostValues, dictionary outputValues Caution :CalculateCosts() must be used at least once before :Learn() void :Learn() Uses the calculated costs up to this point and the associated network's optimizer to backpropagate the network parameters. :Reset() is called when finished. dictionary :GetCost() Returns the cost values for the BackPropagator. See :Reset()'s code if you want to know the structure as it is for internal use.// {Outputs = dictionary, Activations = dictionary} number :GetTotalCost() Takes the square cost of every output node and adds it all up to calculate the total cost number and returns it. This is the prime indicator for how well the network is training with backpropagation.","title":"BackPropagator"},{"location":"documentation/BackPropagator/#backpropagator-class","text":"This class is responsible for backpropagating the associated neural network according to it's set optimizer. BackPropagator .new(NeuralNetwork neuralNetwork) Creates and returns the BackPropagator with the given !neuralNetwork!. void :Reset() Resets all saved gradients and costs. Tuple :CalculateCost(dictionary inputValues,dictionary correctOutputValues) Calculates and saves the cost values for each node when the network is ran with !inputValues! and the output is compared to the given !correctOutputValues!. This function can be called multiple times before :Learn() as it will simply average the cost values over time.// dictionary CostValues, dictionary outputValues Caution :CalculateCosts() must be used at least once before :Learn() void :Learn() Uses the calculated costs up to this point and the associated network's optimizer to backpropagate the network parameters. :Reset() is called when finished. dictionary :GetCost() Returns the cost values for the BackPropagator. See :Reset()'s code if you want to know the structure as it is for internal use.// {Outputs = dictionary, Activations = dictionary} number :GetTotalCost() Takes the square cost of every output node and adds it all up to calculate the total cost number and returns it. This is the prime indicator for how well the network is training with backpropagation.","title":"BackPropagator Class"},{"location":"documentation/FeedforwardNetwork/","text":"FeedforwardNetwork Class This class is responsible for constructing and managing feedforward networks, sometimes also called multilayer perceptron networks. This is the all purpose neural network design known for decades. It can theoretically work in any circumstance, though undoubtably greatly suffers in certain applications where other network types excel due to being one of the simplest network designs. FeedforwardNetwork .new(array inputNamesArray, number numberOfLayers, number numberOfNodesPerLayer, array outputNamesArray, dictionary customSettings) Creates and returns the FeedforwardNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings = { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; HiddenActivationName = \"ReLU\" ; OutputActivationName = \"Sigmoid\" ; Bias = 0 ; LearningRate = 0.3 ; RandomizeWeights = true ; } void :SetBiases(number bias) Sets the bias of every node to !bias!. Inherited from NeuralNetwork : Only unchanged and unmodified functions are listed below. void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"FeedforwardNetwork"},{"location":"documentation/FeedforwardNetwork/#feedforwardnetwork-class","text":"This class is responsible for constructing and managing feedforward networks, sometimes also called multilayer perceptron networks. This is the all purpose neural network design known for decades. It can theoretically work in any circumstance, though undoubtably greatly suffers in certain applications where other network types excel due to being one of the simplest network designs. FeedforwardNetwork .new(array inputNamesArray, number numberOfLayers, number numberOfNodesPerLayer, array outputNamesArray, dictionary customSettings) Creates and returns the FeedforwardNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings = { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; HiddenActivationName = \"ReLU\" ; OutputActivationName = \"Sigmoid\" ; Bias = 0 ; LearningRate = 0.3 ; RandomizeWeights = true ; } void :SetBiases(number bias) Sets the bias of every node to !bias!.","title":"FeedforwardNetwork Class"},{"location":"documentation/FeedforwardNetwork/#inherited-from-neuralnetwork","text":"Only unchanged and unmodified functions are listed below. void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"Inherited from NeuralNetwork:"},{"location":"documentation/GateNetwork/","text":"GateNetwork Class This class is responsible for constructing and managing gate networks. This is a custom type of feedforward network that contains a single hidden node that is specifically managed by several new functions. The point of this type of network is to process values in the simplest way possible while keeping the features of a feedforward network. This is used by LSTM networks. For this documentation, the single hidden node will be called the \"gate node\". GateNetwork .new(array inputNamesArray, array outputNamesArray, dictionary customSettings) Creates and returns the GateNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings = { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; HiddenActivationName = \"ReLU\" ; OutputActivationName = \"Sigmoid\" ; Bias = 0 ; LearningRate = 0.3 ; RandomizeWeights = true ; } Node :GetNode() Returns the gate node. Synapse :GetSynapseWithNodeName(string inputName) Returns the synapse that connects the input node with the name !inputName! and the gate node. number :GetWeight(string inputName) Returns the weight of the synapse that connects the input node with the name !inputName! and the gate node. void :AddWeight(string inputName, number weightDelta) Adds !weightDelta! to the weight of the synapse that connects the input node with the name !inputName! and the gate node. void :SetWeight(string inputName, number weight) Sets weight of the synapse that connects the input node with the name !inputName! and the gate node to !weight!. number :GetBias() Returns the gate node's bias. void :AddBias(number biasDelta) Adds !biasDelta! to the gate node's bias. void :SetBias(number bias) Sets the gate node's bias to !bias!. number :GetLearningRate() Returns the learning rate of the gate node. ActivationFunction :GetActivationFunction() Returns the ActivationFunction object of the gate node. Inherited from FeedforwardNetwork : Only unchanged and unmodified functions are listed below. void :SetBiases(number bias) Sets the bias of every node to !bias!. void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"GateNetwork"},{"location":"documentation/GateNetwork/#gatenetwork-class","text":"This class is responsible for constructing and managing gate networks. This is a custom type of feedforward network that contains a single hidden node that is specifically managed by several new functions. The point of this type of network is to process values in the simplest way possible while keeping the features of a feedforward network. This is used by LSTM networks. For this documentation, the single hidden node will be called the \"gate node\". GateNetwork .new(array inputNamesArray, array outputNamesArray, dictionary customSettings) Creates and returns the GateNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings = { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; HiddenActivationName = \"ReLU\" ; OutputActivationName = \"Sigmoid\" ; Bias = 0 ; LearningRate = 0.3 ; RandomizeWeights = true ; } Node :GetNode() Returns the gate node. Synapse :GetSynapseWithNodeName(string inputName) Returns the synapse that connects the input node with the name !inputName! and the gate node. number :GetWeight(string inputName) Returns the weight of the synapse that connects the input node with the name !inputName! and the gate node. void :AddWeight(string inputName, number weightDelta) Adds !weightDelta! to the weight of the synapse that connects the input node with the name !inputName! and the gate node. void :SetWeight(string inputName, number weight) Sets weight of the synapse that connects the input node with the name !inputName! and the gate node to !weight!. number :GetBias() Returns the gate node's bias. void :AddBias(number biasDelta) Adds !biasDelta! to the gate node's bias. void :SetBias(number bias) Sets the gate node's bias to !bias!. number :GetLearningRate() Returns the learning rate of the gate node. ActivationFunction :GetActivationFunction() Returns the ActivationFunction object of the gate node.","title":"GateNetwork Class"},{"location":"documentation/GateNetwork/#inherited-from-feedforwardnetwork","text":"Only unchanged and unmodified functions are listed below. void :SetBiases(number bias) Sets the bias of every node to !bias!. void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"Inherited from FeedforwardNetwork:"},{"location":"documentation/GeneticAlgorithm/","text":"Abstract GeneticAlgorithm Class This class is responsible for managing a set population of neural networks and breeding them in an efficient manner in order to create better performing networks while following the user's given customizations. This class is abstract and cannot be used directly. abstract GeneticAlgorithm .new(NeuralNetwork neuralNetwork, number popSize, dictionary geneticSettings) Creates and returns the GeneticAlgorithm with the given template !neuralNetwork!, population size as !popSize!, and settings as !geneticSettings!. The settings determine how the algorithm works and is open to customization. The available setting parameters and their default values are below. local geneticSettings = { ScoreFunction = nil ; PostFunction = nil ; HigherScoreBetter = true ; PercentageToKill = 0.5 ; PercentageOfKilledToRandomlySpare = 0.1 ; PercentageOfBestParentToCrossover = 0.6 ; PercentageToMutate = 0.1 ; MutateBestNetwork = false ; PercentageOfCrossedToMutate = 0.5 ; NumberOfNodesToMutate = 2 ; ParameterMutateRange = 4 ; } array :GetPopulation() Returns the population of networks in an array. NeuralNetwork :GetBestNetwork() Returns the best network in the population. void :AddNetwork(NeuralNetwork network) Adds !network! to the population. void :ProcessGeneration(array scoreArray) Completely runs a single generation along with the pre-function and post-function. void :ProcessGenerations(number num) Completely runs !num! number of generations with :ProcessGeneration(). dictionary :GetInfo() Returns a dictionary containing basic status info about the population.// {Generation = number, BestScore = number}","title":"GeneticAlgorithm"},{"location":"documentation/GeneticAlgorithm/#abstract-geneticalgorithm-class","text":"This class is responsible for managing a set population of neural networks and breeding them in an efficient manner in order to create better performing networks while following the user's given customizations. This class is abstract and cannot be used directly. abstract GeneticAlgorithm .new(NeuralNetwork neuralNetwork, number popSize, dictionary geneticSettings) Creates and returns the GeneticAlgorithm with the given template !neuralNetwork!, population size as !popSize!, and settings as !geneticSettings!. The settings determine how the algorithm works and is open to customization. The available setting parameters and their default values are below. local geneticSettings = { ScoreFunction = nil ; PostFunction = nil ; HigherScoreBetter = true ; PercentageToKill = 0.5 ; PercentageOfKilledToRandomlySpare = 0.1 ; PercentageOfBestParentToCrossover = 0.6 ; PercentageToMutate = 0.1 ; MutateBestNetwork = false ; PercentageOfCrossedToMutate = 0.5 ; NumberOfNodesToMutate = 2 ; ParameterMutateRange = 4 ; } array :GetPopulation() Returns the population of networks in an array. NeuralNetwork :GetBestNetwork() Returns the best network in the population. void :AddNetwork(NeuralNetwork network) Adds !network! to the population. void :ProcessGeneration(array scoreArray) Completely runs a single generation along with the pre-function and post-function. void :ProcessGenerations(number num) Completely runs !num! number of generations with :ProcessGeneration(). dictionary :GetInfo() Returns a dictionary containing basic status info about the population.// {Generation = number, BestScore = number}","title":"Abstract GeneticAlgorithm Class"},{"location":"documentation/InputNode/","text":"InputNode Class This class is responsible for managing input nodes. These nodes are heavily limited and effectivelly can only carry a value and fire proceeding nodes as it is simply an input, nothing more. InputNode .new(string name) Creates and returns the InputNode with the given name. void :() Fires the node and fires any node that is at it's output, potentially causing a chain reaction. This function is fired when the InputNode object is called, such as: inputNode :() void :ClearValue() Sets the node's output value to 0. It is not set to nil like with Node because it has to have some sort of output. Inherited from Node : Only unchanged and unmodified functions are listed below. number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetOutputSynapses() Returns the output synapses. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearOutputSynapses() Removes all output synapses.","title":"InputNode"},{"location":"documentation/InputNode/#inputnode-class","text":"This class is responsible for managing input nodes. These nodes are heavily limited and effectivelly can only carry a value and fire proceeding nodes as it is simply an input, nothing more. InputNode .new(string name) Creates and returns the InputNode with the given name. void :() Fires the node and fires any node that is at it's output, potentially causing a chain reaction. This function is fired when the InputNode object is called, such as: inputNode :() void :ClearValue() Sets the node's output value to 0. It is not set to nil like with Node because it has to have some sort of output.","title":"InputNode Class"},{"location":"documentation/InputNode/#inherited-from-node","text":"Only unchanged and unmodified functions are listed below. number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetOutputSynapses() Returns the output synapses. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearOutputSynapses() Removes all output synapses.","title":"Inherited from Node:"},{"location":"documentation/LSTMBackPropagator/","text":"LSTMBackPropagator Class This class is responsible for backpropagating the associated LSTM neural network according to it's set optimizer. It is heavily modified and in places completely changed in order to work with LSTM systems. LSTMBackPropagator .new(NeuralNetwork neuralNetwork) Creates and returns the LSTMBackPropagator with the given !neuralNetwork!. void :Reset() Resets all saved gradients and costs. Caution :CalculateCosts() must be used at least once before :Learn() void :Learn() Uses the calculated costs up to this point and the associated network's optimizer to backpropagate the network parameters. :Reset() is called when finished. Tuple :CalculateCost(dictionary inputValues,dictionary correctOutputValues) Calculates the cost values for each node when the network is ran with !inputValues! and the output is compared to the given !correctOutputValues!. This function can be called multiple times before :Learn() as it will simply average the cost values over time.// dictionary CostValues, dictionary outputValues Inherited from BackPropagator : Only unchanged and unmodified functions are listed below. dictionary :GetCost() Returns the cost values for the BackPropagator. See :Reset()'s code if you want to know the structure as it is for internal use.// {Outputs = dictionary, Activations = dictionary} number :GetTotalCost() Takes the square cost of every output node and adds it all up to calculate the total cost number and returns it. This is the prime indicator for how well the network is training with backpropagation.","title":"LSTMBackPropagator"},{"location":"documentation/LSTMBackPropagator/#lstmbackpropagator-class","text":"This class is responsible for backpropagating the associated LSTM neural network according to it's set optimizer. It is heavily modified and in places completely changed in order to work with LSTM systems. LSTMBackPropagator .new(NeuralNetwork neuralNetwork) Creates and returns the LSTMBackPropagator with the given !neuralNetwork!. void :Reset() Resets all saved gradients and costs. Caution :CalculateCosts() must be used at least once before :Learn() void :Learn() Uses the calculated costs up to this point and the associated network's optimizer to backpropagate the network parameters. :Reset() is called when finished. Tuple :CalculateCost(dictionary inputValues,dictionary correctOutputValues) Calculates the cost values for each node when the network is ran with !inputValues! and the output is compared to the given !correctOutputValues!. This function can be called multiple times before :Learn() as it will simply average the cost values over time.// dictionary CostValues, dictionary outputValues","title":"LSTMBackPropagator Class"},{"location":"documentation/LSTMBackPropagator/#inherited-from-backpropagator","text":"Only unchanged and unmodified functions are listed below. dictionary :GetCost() Returns the cost values for the BackPropagator. See :Reset()'s code if you want to know the structure as it is for internal use.// {Outputs = dictionary, Activations = dictionary} number :GetTotalCost() Takes the square cost of every output node and adds it all up to calculate the total cost number and returns it. This is the prime indicator for how well the network is training with backpropagation.","title":"Inherited from BackPropagator:"},{"location":"documentation/LSTMNetwork/","text":"Unusable As far as I can tell, my LSTM implementation is a complete failure. It does not achieve any measurable success or advantage over feedforward networks and cannot predict even simple number sequences. The internet has little to no readable information on the inner mathematical workings of LSTMs since few have the interest of creating new libraries, so I have no way of knowing how a true LSTM behaves internally. Sorry for the inconvenience. This class is responsible for constructing and managing LSTM (long short term memory) networks. This type of network is specialized for extrapolating a datapoint that would continue the series of given datapoints. A common application of this is text generation, though it can be applied to any scenario where there is a sequence that needs to be continued. LSTMNetwork .new(array inputNamesArray, number numberOfLSTMLayers, number numberOfLSTMNodesPerLayer, array outputNamesArray, dictionary customSettings) Creates and returns the LSTMNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings = { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; HiddenActivationName = \"ReLU\" ; OutputActivationName = \"Sigmoid\" ; Bias = 0 ; LearningRate = 0.3 ; NumOfInputsForLSTMUnits = { Default = 1 }; MakeDenseLayer = true ; MakeDirectOutput = false ; RandomizeWeights = true ; } Inherited from NeuralNetwork : Only unchanged and unmodified functions are listed below. void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"LSTMNetwork"},{"location":"documentation/LSTMNetwork/#inherited-from-neuralnetwork","text":"Only unchanged and unmodified functions are listed below. void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"Inherited from NeuralNetwork:"},{"location":"documentation/LSTMNode/","text":"LSTMNode Class This class is responsible for managing all LSTM nodes. The difference between normal hidden nodes and LSTM nodes is the existance of a connection between every LSTM node in the same layer, known as the previous/next LSTM nodes. Each LSTM node also has 4 gate networks necessary for it's function, making it considerably larger than normal nodes. LSTMNode .new(string activName, number bias, number learningRate, LSTMNode prevLSTMNode = nil, LSTMNode nextLSTMNode = nil, number numOfInputs=1) Creates and returns the LSTMNode with the given activation function, bias, learning rate, previous/next LSTM nodes (if they exist), and number of inputs. number :CalculateValue() Calculates the node's output value and updates the cost data. void :ClearValue() Resets the node's output value and gate networks. number :GetCellState() Returns the cell state. number :GetPrevCellState() Returns the cell state of the previous LSTM node if it exists. number :GetPrevHiddenState() Returns the hidden state of the previous LSTM node if it exists. void :SetCellState(number cellState) Sets the cell state to !cellstate!. LSTMNode :GetNextLSTMNode() Returns the next LSTM node if it exists. void :SetNextLSTMNode(LSTMNode nextLSTMNode) Sets the next LSTM node to !nextLSTMNode!. LSTMNode :GetPrevLSTMNode() Returns the previous LSTM node if it exists. void :SetPrevLSTMNode(LSTMNode prevLSTMNode) Sets the previous LSTM node to !prevLSTMNode!. ActivationFunction :GetActivationFunction() Returns the node's Tanh ActivationFunction. void :AddRandomNoise(number min, number max) Adds a random noise with the minimum !min! and maximum !max! to the gate networks' parameters in the node. Inherited from Node : Only unchanged and unmodified functions are listed below. void :() Fires the node and fires any node that is at it's output, potentially causing a chain reaction. This function is fired when the Node object is called, such as: node :() number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. void :AddInputSynapse(Synapse inputSynapse) Adds the synapse !inputSynapse! as an input synapse. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetInputSynapses() Returns the input synapses. array :GetOutputSynapses() Returns the output synapses. void :RemoveInputSynapse(Synapse inputSynapse) Removes the input synapse !inputSynapse!. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearInputSynapses() Removes all input synapses. void :ClearOutputSynapses() Removes all output synapses. void :SetLearningRate(number learningRate) Sets the node's learning rate to !learningRate!. number :GetLearningRate() Returns the node's learning rate. ActivationFunction :GetActivationFunction() Returns the node's ActivationFunction object.","title":"LSTMNode"},{"location":"documentation/LSTMNode/#lstmnode-class","text":"This class is responsible for managing all LSTM nodes. The difference between normal hidden nodes and LSTM nodes is the existance of a connection between every LSTM node in the same layer, known as the previous/next LSTM nodes. Each LSTM node also has 4 gate networks necessary for it's function, making it considerably larger than normal nodes. LSTMNode .new(string activName, number bias, number learningRate, LSTMNode prevLSTMNode = nil, LSTMNode nextLSTMNode = nil, number numOfInputs=1) Creates and returns the LSTMNode with the given activation function, bias, learning rate, previous/next LSTM nodes (if they exist), and number of inputs. number :CalculateValue() Calculates the node's output value and updates the cost data. void :ClearValue() Resets the node's output value and gate networks. number :GetCellState() Returns the cell state. number :GetPrevCellState() Returns the cell state of the previous LSTM node if it exists. number :GetPrevHiddenState() Returns the hidden state of the previous LSTM node if it exists. void :SetCellState(number cellState) Sets the cell state to !cellstate!. LSTMNode :GetNextLSTMNode() Returns the next LSTM node if it exists. void :SetNextLSTMNode(LSTMNode nextLSTMNode) Sets the next LSTM node to !nextLSTMNode!. LSTMNode :GetPrevLSTMNode() Returns the previous LSTM node if it exists. void :SetPrevLSTMNode(LSTMNode prevLSTMNode) Sets the previous LSTM node to !prevLSTMNode!. ActivationFunction :GetActivationFunction() Returns the node's Tanh ActivationFunction. void :AddRandomNoise(number min, number max) Adds a random noise with the minimum !min! and maximum !max! to the gate networks' parameters in the node.","title":"LSTMNode Class"},{"location":"documentation/LSTMNode/#inherited-from-node","text":"Only unchanged and unmodified functions are listed below. void :() Fires the node and fires any node that is at it's output, potentially causing a chain reaction. This function is fired when the Node object is called, such as: node :() number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. void :AddInputSynapse(Synapse inputSynapse) Adds the synapse !inputSynapse! as an input synapse. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetInputSynapses() Returns the input synapses. array :GetOutputSynapses() Returns the output synapses. void :RemoveInputSynapse(Synapse inputSynapse) Removes the input synapse !inputSynapse!. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearInputSynapses() Removes all input synapses. void :ClearOutputSynapses() Removes all output synapses. void :SetLearningRate(number learningRate) Sets the node's learning rate to !learningRate!. number :GetLearningRate() Returns the node's learning rate. ActivationFunction :GetActivationFunction() Returns the node's ActivationFunction object.","title":"Inherited from Node:"},{"location":"documentation/Momentum/","text":"Momentum Class This class is responsible for the Momentum optimizer. Momentum .new(number momentumConstant = 0.1) Creates and returns the Momentum optimizer with the given momentum constant. Inherited from Optimizer : Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Momentum"},{"location":"documentation/Momentum/#momentum-class","text":"This class is responsible for the Momentum optimizer. Momentum .new(number momentumConstant = 0.1) Creates and returns the Momentum optimizer with the given momentum constant.","title":"Momentum Class"},{"location":"documentation/Momentum/#inherited-from-optimizer","text":"Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Inherited from Optimizer:"},{"location":"documentation/NeuralNetwork/","text":"Abstract NeuralNetwork Class This class is responsible for the majority of the functionality behind all present neural networks. Though it does not construct anything, it does manage most of what a neural network is. This class is abstract and thus cannot be used directly. abstract NeuralNetwork .new(dictionary customSettings) Creates and returns the NeuralNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; } void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"NeuralNetwork"},{"location":"documentation/NeuralNetwork/#abstract-neuralnetwork-class","text":"This class is responsible for the majority of the functionality behind all present neural networks. Though it does not construct anything, it does manage most of what a neural network is. This class is abstract and thus cannot be used directly. abstract NeuralNetwork .new(dictionary customSettings) Creates and returns the NeuralNetwork according to the given settings as !customSettings!. The settings determine how the network will perform and function and is open to customization. The available setting parameters and their default values are below. local customSettings { Optimizer = StochasticGradientDescent . new (); SoftMax = false ; } void :ConnectNodes(Node inNode, Node outNode, bool checkOveride=false) Creates a synapse for the 2 nodes with !inNode! as the input and !outNode! as the output. If !checkOveride! is true, this function will not check if the synapse between these 2 nodes already exists; this is purely for internal performance purposes where the check is unnecessary. Do not set this to true if you are experimenting unless you're confident. dictionary :(dictionary inputValues, bool doNotClearOtherInputValues=false) Propagates the network with the given !inputValues! if provided. If not, the network will run with the input values already set previously. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. This function is fired when the NeuralNetwork object is called, such as: neuralNetwork :({ Input1 = 0.5 , Input2 = - 0.2 }) dictionary :GetOutputValues() Returns the current output values. void :SetInputValues(dictionary inputValues, bool doNotClearOtherInputValues=false) Sets the given input values as !inputValues! into the input nodes. If !doNotClearOtherInputValues! is true, any inputs that are missing from the !inputValues! dictionary are set to 0. void :ClearInputValues() Sets all input values of the input nodes to 0. void :ClearValues() Sets all the output values of every functional node to 0. array :GetNodes() Returns every node in the network in an unreliable order. array :GetInputNodes() Returns the input nodes in the network in the order they were assigned by the user upon network creation. void :AddInputNode(InputNode inputNode) Adds input node !inputNode! to the network. array :GetOutputNodes() Returns the output nodes in the network in the order they were assigned by the user upon network creation. void :AddOutputNode(OutputNode outputNode) Adds output node !outputNode! to the network. array :GetHiddenNodes() Returns the hidden nodes in the network in the order they were created (first node to last node of first layer, first node to last node of second layer, etc). void :AddHiddenNode(Node node) Adds the hidden node !node! to the network. array :GetFunctionNodes() Returns the functional nodes in the network. Functional nodes, in my definition, are nodes that have a working bias and weight set. Thus, hidden nodes and output nodes are all functional nodes. void :AddNode(Node node) Adds the node !node! to the network. BackPropagator :GetBackPropagator() Returns the BackPropagator object for this network. void :RandomizeWeights(number min=-0.5, number max=0.5) Randomizes all weights in the network with the given minimum !min! and maximum !max! values. Optimizer :GetOptimizer() Returns the Optimizer object the network is using. void :AddRandomNoise(number min, number max) Adds a random noise to every parameter in the network with the given minimum !min! and maximum !max! values.","title":"Abstract NeuralNetwork Class"},{"location":"documentation/Node/","text":"Node Class This class is responsible for managing all hidden nodes and most of the functionality behind other node types that inherit it. Node .new(string activeName, number bias=0, number learningRate=0.1) Creates and returns the Node with the given activation function, bias, and learning rate. void :() Fires the node and fires any node that is at it's output, potentially causing a chain reaction. This function is fired when the Node object is called, such as: node :() number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. number :CalculateValue() Calculates and sets the node's new output value while also returning it. void :ClearValue() Resets the node's output value. void :AddInputSynapse(Synapse inputSynapse) Adds the synapse !inputSynapse! as an input synapse. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetInputSynapses() Returns the input synapses. array :GetOutputSynapses() Returns the output synapses. void :RemoveInputSynapse(Synapse inputSynapse) Removes the input synapse !inputSynapse!. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearInputSynapses() Removes all input synapses. void :ClearOutputSynapses() Removes all output synapses. void :SetBias(number bias) Sets the node's bias to !bias!. number :GetBias() Returns the node's bias. void :AddBias(number biasDelta) Adds !biasDelta! to the noder's bias. void :SetLearningRate(number learningRate) Sets the node's learning rate to !learningRate!. number :GetLearningRate() Returns the node's learning rate. ActivationFunction :GetActivationFunction() Returns the node's ActivationFunction object. void :AddRandomNoise(number min, number max) Adds random noise to the node's bias and input weights with the given minimum !min! and maximum !max!.","title":"Node"},{"location":"documentation/Node/#node-class","text":"This class is responsible for managing all hidden nodes and most of the functionality behind other node types that inherit it. Node .new(string activeName, number bias=0, number learningRate=0.1) Creates and returns the Node with the given activation function, bias, and learning rate. void :() Fires the node and fires any node that is at it's output, potentially causing a chain reaction. This function is fired when the Node object is called, such as: node :() number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. number :CalculateValue() Calculates and sets the node's new output value while also returning it. void :ClearValue() Resets the node's output value. void :AddInputSynapse(Synapse inputSynapse) Adds the synapse !inputSynapse! as an input synapse. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetInputSynapses() Returns the input synapses. array :GetOutputSynapses() Returns the output synapses. void :RemoveInputSynapse(Synapse inputSynapse) Removes the input synapse !inputSynapse!. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearInputSynapses() Removes all input synapses. void :ClearOutputSynapses() Removes all output synapses. void :SetBias(number bias) Sets the node's bias to !bias!. number :GetBias() Returns the node's bias. void :AddBias(number biasDelta) Adds !biasDelta! to the noder's bias. void :SetLearningRate(number learningRate) Sets the node's learning rate to !learningRate!. number :GetLearningRate() Returns the node's learning rate. ActivationFunction :GetActivationFunction() Returns the node's ActivationFunction object. void :AddRandomNoise(number min, number max) Adds random noise to the node's bias and input weights with the given minimum !min! and maximum !max!.","title":"Node Class"},{"location":"documentation/NodeLayer/","text":"NodeLayer Class This class is responsible for housing several nodes into 1 tangible layer. Probably should've been a simple array but I thought I will need some functions here eventually. NodeLayer .new() Creates and returns the NodeLayer. void :AddNodes(Tuple ...) Adds the given tuple of nodes into the layer. array :GetNodes() Returns the array of nodes in the layer.","title":"NodeLayer"},{"location":"documentation/NodeLayer/#nodelayer-class","text":"This class is responsible for housing several nodes into 1 tangible layer. Probably should've been a simple array but I thought I will need some functions here eventually. NodeLayer .new() Creates and returns the NodeLayer. void :AddNodes(Tuple ...) Adds the given tuple of nodes into the layer. array :GetNodes() Returns the array of nodes in the layer.","title":"NodeLayer Class"},{"location":"documentation/Optimizer/","text":"Abstract Optimizer Class This class is responsible for the base of all optimizers. Optimizers are functions that determine the stepping size of backpropagation, each unique optimizer having a different algorithm at play. Each one has it's strengths and weaknesses. This class is abstract and thus cannot be used directly. abstract Optimizer .new() Creates and returns the Optimizer. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Optimizer"},{"location":"documentation/Optimizer/#abstract-optimizer-class","text":"This class is responsible for the base of all optimizers. Optimizers are functions that determine the stepping size of backpropagation, each unique optimizer having a different algorithm at play. Each one has it's strengths and weaknesses. This class is abstract and thus cannot be used directly. abstract Optimizer .new() Creates and returns the Optimizer. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Abstract Optimizer Class"},{"location":"documentation/OutputNode/","text":"OutputNode Class This class is responsible for managing output nodes. These nodes are effectively the same as hidden nodes expect that their output values are specifically saved as the network's output values. They are always on the last layer. OutputNode .new(string activName, string name, number bias, number learningRate) Creates and returns the OutputNode with the given activation function, name, bias, and learning rate. void :() Fires the OutputNode and saves it's output value to the network's output values. This function is fired when the OutputNode object is called, such as: outputNode :() void :SetDirectOutput(bool bool) Sets whether or not the OutputNode should use it's activation function and bias or not. This is useful when you want to only use the hidden nodes and don't want the output nodes to interfere. Inherited from Node : Only unchanged and unmodified functions are listed below. number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. number :CalculateValue() Calculates and sets the node's new output value while also returning it. void :ClearValue() Resets the node's output value. void :AddInputSynapse(Synapse inputSynapse) Adds the synapse !inputSynapse! as an input synapse. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetInputSynapses() Returns the input synapses. array :GetOutputSynapses() Returns the output synapses. void :RemoveInputSynapse(Synapse inputSynapse) Removes the input synapse !inputSynapse!. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearInputSynapses() Removes all input synapses. void :ClearOutputSynapses() Removes all output synapses. void :SetBias(number bias) Sets the node's bias to !bias!. number :GetBias() Returns the node's bias. void :AddBias(number biasDelta) Adds !biasDelta! to the noder's bias. void :SetLearningRate(number learningRate) Sets the node's learning rate to !learningRate!. number :GetLearningRate() Returns the node's learning rate. ActivationFunction :GetActivationFunction() Returns the node's ActivationFunction object. void :AddRandomNoise(number min, number max) Adds random noise to the node's bias and input weights with the given minimum !min! and maximum !max!.","title":"OutputNode"},{"location":"documentation/OutputNode/#outputnode-class","text":"This class is responsible for managing output nodes. These nodes are effectively the same as hidden nodes expect that their output values are specifically saved as the network's output values. They are always on the last layer. OutputNode .new(string activName, string name, number bias, number learningRate) Creates and returns the OutputNode with the given activation function, name, bias, and learning rate. void :() Fires the OutputNode and saves it's output value to the network's output values. This function is fired when the OutputNode object is called, such as: outputNode :() void :SetDirectOutput(bool bool) Sets whether or not the OutputNode should use it's activation function and bias or not. This is useful when you want to only use the hidden nodes and don't want the output nodes to interfere.","title":"OutputNode Class"},{"location":"documentation/OutputNode/#inherited-from-node","text":"Only unchanged and unmodified functions are listed below. number :GetValue() Returns the node's output value if calculated. If not, calculates it and returns the new output value. void :SetValue(number value) Sets the node's output value to !value!. number :CalculateValue() Calculates and sets the node's new output value while also returning it. void :ClearValue() Resets the node's output value. void :AddInputSynapse(Synapse inputSynapse) Adds the synapse !inputSynapse! as an input synapse. void :AddOutputSynapse(Synapse outputSynapse) Adds the synapse !outputSynapse! as an output synapse. array :GetInputSynapses() Returns the input synapses. array :GetOutputSynapses() Returns the output synapses. void :RemoveInputSynapse(Synapse inputSynapse) Removes the input synapse !inputSynapse!. void :RemoveOutputSynapse(Synapse outputSynapse) Removes the output synapse !inputSynapse!. void :ClearInputSynapses() Removes all input synapses. void :ClearOutputSynapses() Removes all output synapses. void :SetBias(number bias) Sets the node's bias to !bias!. number :GetBias() Returns the node's bias. void :AddBias(number biasDelta) Adds !biasDelta! to the noder's bias. void :SetLearningRate(number learningRate) Sets the node's learning rate to !learningRate!. number :GetLearningRate() Returns the node's learning rate. ActivationFunction :GetActivationFunction() Returns the node's ActivationFunction object. void :AddRandomNoise(number min, number max) Adds random noise to the node's bias and input weights with the given minimum !min! and maximum !max!.","title":"Inherited from Node:"},{"location":"documentation/ParamEvo/","text":"ParamEvo Class This class is responsible for managing a set population of neural networks and breeding them in an efficient manner in order to create better performing networks' parameters (weights and biases) while following the user's given customizations. ParamEvo .new(NeuralNetwork neuralNetworkTemp, number popSize, dictionary geneticSettings) Creates and returns the ParamEvo with the given template !neuralNetwork!, population size as !popSize!, and settings as !geneticSettings!. The settings determine how the algorithm works and is open to customization. The available setting parameters and their default values are below. local geneticSettings = { ScoreFunction = nil ; PostFunction = nil ; HigherScoreBetter = true ; PercentageToKill = 0.5 ; PercentageOfKilledToRandomlySpare = 0.1 ; PercentageOfBestParentToCrossover = 0.6 ; PercentageToMutate = 0.1 ; MutateBestNetwork = false ; PercentageOfCrossedToMutate = 0.5 ; NumberOfNodesToMutate = 2 ; ParameterMutateRange = 4 ; ParameterNoiseRange = 0.01 ; } Inherited from GeneticAlgorithm : Only unchanged and unmodified functions are listed below. array :GetPopulation() Returns the population of networks in an array. NeuralNetwork :GetBestNetwork() Returns the best network in the population. void :AddNetwork(NeuralNetwork network) Adds !network! to the population. void :ProcessGeneration(array scoreArray) Completely runs a single generation along with the pre-function and post-function. void :ProcessGenerations(number num) Completely runs !num! number of generations with :ProcessGeneration(). dictionary :GetInfo() Returns a dictionary containing basic status info about the population.// {Generation = number, BestScore = number}","title":"ParamEvo"},{"location":"documentation/ParamEvo/#paramevo-class","text":"This class is responsible for managing a set population of neural networks and breeding them in an efficient manner in order to create better performing networks' parameters (weights and biases) while following the user's given customizations. ParamEvo .new(NeuralNetwork neuralNetworkTemp, number popSize, dictionary geneticSettings) Creates and returns the ParamEvo with the given template !neuralNetwork!, population size as !popSize!, and settings as !geneticSettings!. The settings determine how the algorithm works and is open to customization. The available setting parameters and their default values are below. local geneticSettings = { ScoreFunction = nil ; PostFunction = nil ; HigherScoreBetter = true ; PercentageToKill = 0.5 ; PercentageOfKilledToRandomlySpare = 0.1 ; PercentageOfBestParentToCrossover = 0.6 ; PercentageToMutate = 0.1 ; MutateBestNetwork = false ; PercentageOfCrossedToMutate = 0.5 ; NumberOfNodesToMutate = 2 ; ParameterMutateRange = 4 ; ParameterNoiseRange = 0.01 ; }","title":"ParamEvo Class"},{"location":"documentation/ParamEvo/#inherited-from-geneticalgorithm","text":"Only unchanged and unmodified functions are listed below. array :GetPopulation() Returns the population of networks in an array. NeuralNetwork :GetBestNetwork() Returns the best network in the population. void :AddNetwork(NeuralNetwork network) Adds !network! to the population. void :ProcessGeneration(array scoreArray) Completely runs a single generation along with the pre-function and post-function. void :ProcessGenerations(number num) Completely runs !num! number of generations with :ProcessGeneration(). dictionary :GetInfo() Returns a dictionary containing basic status info about the population.// {Generation = number, BestScore = number}","title":"Inherited from GeneticAlgorithm:"},{"location":"documentation/RMSprop/","text":"RMSprop Class This class is responsible for the RMSprop optimizer. RMSprop .new(number decayConstant = 0.9, number epsilon = 10^-8) Creates and returns the RMSprop optimizer with the given decay constant and epsilon. Inherited from Optimizer : Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"RMSprop"},{"location":"documentation/RMSprop/#rmsprop-class","text":"This class is responsible for the RMSprop optimizer. RMSprop .new(number decayConstant = 0.9, number epsilon = 10^-8) Creates and returns the RMSprop optimizer with the given decay constant and epsilon.","title":"RMSprop Class"},{"location":"documentation/RMSprop/#inherited-from-optimizer","text":"Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Inherited from Optimizer:"},{"location":"documentation/StochasticGradientDescent/","text":"StochasticGradientDescent Class This class is responsible for the AdaGrad optimizer. StochasticGradientDescent .new() Creates and returns the StochasticGradientDescent optimizer. Inherited from Optimizer : Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"StochasticGradientDescent"},{"location":"documentation/StochasticGradientDescent/#stochasticgradientdescent-class","text":"This class is responsible for the AdaGrad optimizer. StochasticGradientDescent .new() Creates and returns the StochasticGradientDescent optimizer.","title":"StochasticGradientDescent Class"},{"location":"documentation/StochasticGradientDescent/#inherited-from-optimizer","text":"Only unchanged and unmodified functions are listed below. void :SetNetwork(NeuralNetwork network) Sets the optimizer's network to !network!.","title":"Inherited from Optimizer:"},{"location":"documentation/Synapse/","text":"Synapse Class This class is responsible for bridging the connection between 2 nodes, housing the weight, and providing functions to easily manage it. Synapse .new(Node input,Node output,number weight) Creates and returns the Synapse with the given input/output nodes and weight. !weight! defaults to 0. void :call() Propagates the output node. This function is fired when the Synapse object is called, such as: synapse :() number :GetValue() Returns the output of the input node. void :SetInputNode(Node inputNode) Sets the input node. Node :GetInputNode() Returns the input node. void :SetOutputNode(Node outputNode) Sets the output node. Node :GetOutputNode() Returns the output node. number :GetWeight() Returns the weight. void :AddWeight(number weightDelta) Adds !weightDelta! to the current weight. void :SetWeight(number weight) Sets the weight.","title":"Synapse"},{"location":"documentation/Synapse/#synapse-class","text":"This class is responsible for bridging the connection between 2 nodes, housing the weight, and providing functions to easily manage it. Synapse .new(Node input,Node output,number weight) Creates and returns the Synapse with the given input/output nodes and weight. !weight! defaults to 0. void :call() Propagates the output node. This function is fired when the Synapse object is called, such as: synapse :() number :GetValue() Returns the output of the input node. void :SetInputNode(Node inputNode) Sets the input node. Node :GetInputNode() Returns the input node. void :SetOutputNode(Node outputNode) Sets the output node. Node :GetOutputNode() Returns the output node. number :GetWeight() Returns the weight. void :AddWeight(number weightDelta) Adds !weightDelta! to the current weight. void :SetWeight(number weight) Sets the weight.","title":"Synapse Class"},{"location":"documentation/TEMP/","text":"TEMP Class This class is responsible for","title":"**TEMP Class**"},{"location":"documentation/TEMP/#temp-class","text":"This class is responsible for","title":"TEMP Class"}]}